#+title: Todo

* tagging
** status
Status(0, "eyebrows", "visible")
Status(0, "_", "implied")
** attributes
subject, aspect, attribute, value
the post argument is implicit
Attr(0, "hair", "colour", "red")
** action
Act(0, 1, {action: hit})

#+begin_src js

{
  "subjects": [
    {
      "id": 0,
      "attributes": {
        "type": "character",
        "hair": {"colour": "pink", "length": "long"},
        "face": {"expression": "smile"},
        "outfit": {"items": [{"kind": "habit", "colour": "black"}]
        }
      }
    },
    {
      "id": 1,
      "attributes": {
        "type": "character",
        "hair": {"colour": "red", "length": "short"}
      }
    },
    {
      "id": 2,
      "attributes": {
        "type": "object",
        "categories": ["food", "fruit"],
        "identity": "apple"
      }
    }
  ],
  "relations": [
    {
      "action": "bite",
      "agent": 0,
      "patient": 1,
      "data": {"target": "cheek"}
    },
    {
      "action": "bite",
      "agent": 1,
      "patient": 2,
      "data": {}
    },
    {
      "action": "rest",
      "agent": 1,
      "patient": 1,
      "data": {"resting": "arm", "support": "stomach"}
    }
  ]
}

/*
  obviously i won't do this in jq, it'll be converted to sql

  usual queries

  1. black_hair
  any(black_hair) // apply any when a raw tag is given
  any(is(type: person) && has_attr(name: hair, colour: black)) // look up the definition of black_hair and constraints (is person)
  select(.subjects[]|any(.attributes.hair?.colour? == "black"))

  2. all(black_hair)
     similar derivation

  3. self_* tags
  self(rest)
  any(self(action: rest))
  any([X:character] has_action(name: rest, agent: X, patient: X))

  select(.relations[]|any(.action == "rest" and .agent == .patient))
  select(.relations[]|select(.action == "rest")|any(.agent == .patient))

  4. mutual_* tags
  mutual(bite)
  any(mutual(action: bite))
  any([X,Y:character] has_action(name: rest, agent: X, patient: Y), has_action(name: bite, agent: Y, patient: X))

  def f: any(.action == whatever and .agent == .[0] and .patient == .[1])
  select(.relations
    | map(
        select(.action == "bite") | select(.agent != .patient) | [.agent, .patient]
      )
    | [., reverse]
    | .[0] - (.[0] - .[1])
    )

,*/
#+end_src

* short-term
** generate doc data as json
** /books/:id/view/diff-since endpoint?
** replace hit counter by density
** DONE real header!
** don't construct regexes in hot loops
** unwrap
** associate mnemes to kanji, readings
*** "average status of all words using this reading" can be used to colour spans in the header, say
** amoghavajra
** symlink migration dirs
** DONE have the related sentences query also produce auxiliary stats like num occs per book, num books omitted, etc
** DONE sort by books with most hits first (most "relevant")
will allow pruning harder because you can group in the select from the mv and drop all but the first num_books' rows
** WAIT cleverly prune book titles: chop off the first を or は or something
** show status with little blob rather than an underline?
** group defs by tags and dict
JMdict v1 vt
  - a
  - b
Oubunsha
  - a
  - b

* delayed unpoly loading
** problem: how do you make one link make multiple independent requests?
*** solution 1: have the server return a skeleton with up-poll up-interval=1 on every fragment
this would then pull from endpoints that have no up-poll on them
advantage: no js
disadvantage: extra server roundtrip
*** solution 2: up.compiler

* bugs
** sf collision
szr=# select t.* from tokens t where t.surface_form_id = '00000000-0000-0000-0028-4b8204022241';
you get both 忘れ and 忘れる

** 　鼻の孔の途中でとまった血が気味悪く後戻りしてゆく。息をすると、むずむずいう。
ゆう
** 　葬式の明る日
明くる日
*** TODO integrate some kind of "okuri swallowing" into the tokeniser

* random stuff
** wiktionary has per-character ruby
also has jukujikun (see 大人買い for instance)
https://kaikki.org/dictionary/All%20languages%20combined/meaning/%E6%AC%A0/%E6%AC%A0%E7%89%87/%E6%AC%A0%E7%89%87.html

* architecture
** TODO handle images!
** TODO extend the tokeniser to handle skippable things
** DONE break lemma type into lemma_id + fk to a variant type (id, lemma_id, spelling, reading)
** DONE surface forms must be associated to a variant_id rather than a lemma_id
*** alternatively variant (id, spelling, reading) + lemmas_variants (lemma_id, variant_id) + etc
** kanji table
** reading table
** id, kanji_id, reading_id join table
*** this modeling additionally allows us to link "similar" kanji: which ones appear with identical readings in the same lemmas?
** variant, index (for ordering of links), kanji_reading_id table
** variant has fsrs card (??) so this will allow link computation
*** would be a first form of srs linking/influence
*** alt, we could start with srs on lemmas and join through the variant table
** DONE get rid of sqlx migrations and instead have something like
#+begin_src rust
Def::create_table();
Def::create_indexes();
Def::drop_indexes();
#+end_src
etc to make consistency with "migrations" easier given we are procedurally dropping and recreating indexes everywhere

* small features
** read yomichan dict names from the index.json

* features
** TODO don't search name dictionaries when the parser doesn't think it's a name, or at least deprioritise them in some kind of streaming query result thing
** TODO match pos when doing dictionary search
** DONE smaller crates
** DONE move to pg
** TODO yomichan: use dict name from index.json
** TODO fix the handling of fallbacks when searching for word info
Ideally you'd want something that keeps the first of all the options that maximises definition count. In particular, the first that gets you 0.
** TODO book "collections" and starred sentence display being limited to collections
** TODO replace the css for every instance of a word when doing a review
*** although with js you could just change css for one class
** TODO ping the reader when a review comes up :)
** TODO parallelise epub reading
** symbols, readings, symbol_readings (id, language, symbol, reading)
** term_symbol_readings (id, term_id, symbol_reading_id)
